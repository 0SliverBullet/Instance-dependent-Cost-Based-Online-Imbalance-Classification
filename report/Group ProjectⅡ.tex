\documentclass{article}
\usepackage{multirow}

%\usepackage{PRIMEarxiv}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder
\usepackage{algorithm2e}
\usepackage{subfigure}
%\usepackage{subcaption}
\usepackage{amsmath}
%流程图
\usepackage{tikz}
\usetikzlibrary{arrows, shapes, chains}
\tikzstyle{startstop} = [rectangle,rounded corners, minimum width=3cm,minimum height=1cm,text centered, draw=black,fill=red!30]
\tikzstyle{io} = [trapezium, trapezium left angle = 70,trapezium right angle=110,minimum width=3cm,minimum height=1cm,text centered,draw=black,fill=blue!30]
\tikzstyle{process} = [rectangle,minimum width=3cm,minimum height=1cm,text centered,text width =3cm,draw=black,fill=orange!30]
\tikzstyle{decision} = [diamond,minimum width=3cm,minimum height=1cm,text centered,draw=black,fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]



\newcommand\subtitle[1]{{\small #1}}
\usepackage[colorinlistoftodos]{todonotes}

\hypersetup{
	colorlinks = true, % 使用彩色链接
	linkcolor = blue,  % 链接的颜色
	anchorcolor = blue, % 锚点链接的颜色
	citecolor = blue,   % 引用链接的颜色
	urlcolor = blue,    % URL 链接的颜色
	pdfborder = {0 0 0}, % 取消链接的边框
}

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}
%opening
\title{Group Project}
\author{}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

\section{Related work}

\section{Problem Formulation}
Generally, considering a data stream $\{X_t\}$, where t $\in$ $\{$1, 2, · · · $\}$ is the test time step and $X_t$ $\in$ $\mathbb{R}^d$ denotes the d-dimensional data feature arrived at time step t, the corresponding $y_t$ to denote the true label of $X_t$ and $y_t$ $\in$ $\{$1, \dots, c$\}$ for c $\geq$ 2, where c is the number of classes, however, the number of samples in each category in the data stream is uneven, i.e. data skewing occurs, online imbalance learning follows the conventional “test-then-train" online learning process: test sample arrives strictly one by one, $X_t$ arrived at t, the aim of online imbalance learning is to predict its label with the latest model as 

$$\hat{y}_t =\Theta_{t-1}(X_t),$$ 

where $\Theta_{t-1}$ is the model trained after time t-1. Then, one can obtain the true label $y_t$ before t + 1, and the new training sample $(X_t, y_t)$ is used to update model $\Theta_{t-1}(\cdot)$  to $\Theta_{t}(\cdot)$. 

\section{Proposed algorithm: Instance Dependent Cost Online Classification}

In this section, we design heuristics instance-dependent cost in online learning. To simplify cost matrix, we only consider the misclassification cost for each class(degree of freedom: $k^2-k$ down to $k-1$) and the misclassification cost for each instance (degree of freedom: $n\times(k^2-k)$ down to $n\times(k-1)$) , where k is number of classes, n is number of instances. 

\subsection{The original structure of the proposed algorithm}

For each prediction, if our classifier predict correctly, we continue to train the classifier by fitting this instance by the class-dependent cost; Otherwise, i.e. the current classifier can not predict this instance correctly, the instance is difficult to the classifier that it can not handle, our current classifier should pay more attention to this instance. We continue to train the classifier by fitting this instance by the heuristics instance-dependent cost.

Note that in algorithm 1 we take the prediction error into consideration when calculating the heuristics instance-dependent cost. The prediction error term focus more on the instance that p nears to 0.5 but is misclassified as the opposite class. For such instances, our current classifier is almost able to predict correctly, so we give them a higher prediction error cost. However, for the instance that p nears to 0 or 1 but is misclassified as the opposite class, there are two possible reasons: 1) our current classifier is still weak 2) the instance is a noise. For such instance, we give them a lower prediction error cost to prevent overfitting or learning from noise. By receiving the feedback from prediction, the model is trained to strengthen the ability to correctly classify samples that are easy to classify incorrectly, thereby improving the overall performance of the classifier step by step.


\begin{algorithm}
	\caption{Heuristics Instance-dependent Cost Online Classification 1}
	\KwIn{Input data, Input Labels}
	\KwOut{Prediction}
	\While{Have more samples}{
		datum, label <- next sample
		
		prediction label $\hat{y}$  <-  Predict by current classifier
		
		y <- the true label of this sample
		
		current ratio of Class y: $CRC_y$ <- $\frac{\# \ of\ current samples\ with\ label\ y}{\# \ of\ all\ current\ samples}$ 
		
		Class-dependent Cost: $CDC_y$ <- $\frac{1}{CRC_y}$
		
		
		\eIf{Prediction is correct}{
			train the classifier by fitting this instance with the class-dependent cost
		}{
			
			prediction error <- $\alpha e^{\beta (1-y(1-p)-(1-y)p)}$ 
			
			Instance-dependent Cost: $IDC$ <- $CDC_y$ + prediction error 
			
			
			train the classifier by fitting this instance with the heuristics instance-dependent cost
		}
		
		
	}
\end{algorithm}


\subsection{Change the structure of the proposed algorithm}

We have modified the structure of the original algorithm in two aspects, one is the cost of using feedback based on instance prediction results each timestep, and the other one is the modification in the definition of prediction error term.

For the first aspect, we do not distinguish between correctly classified and incorrectly classified instance, using the same definition of instance-dependent cost as the cost for each instance. Since we found that, if the classifier correctly classifies each instance, is will degenerate into class-dependent cost classifier, which shows no obvious difference between class-dependent cost classifier and instance-dependent cost classifier.


For the second aspect, the prediction error is redefined as: 

\begin{equation}
	\text{prediction error} = (1 - p_t)^\alpha,
\end{equation}
 where $p_t$ is the classification probability as the true label, defined as:
 
 \begin{equation}
 	p_t = 
 	\begin{cases}
 		p, \ \text{if y=1}\\
 		1-p, \ \text{if y=0}\\
 	\end{cases}
 \end{equation}
  In the above y $\in$ $\{ 0, 1\}$ specifies the ground-truth class and p $\in$ [0, 1] is the model's estimated probability for the class with label y = 1.

Then the instance-dependent cost is redefined as 
 \begin{equation}
 	\text{instance-dependent cost}= (1+ \text{prediction error})\times \text{class-dependent cost}
 \end{equation}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=1\linewidth]{"in"}
	\caption{instance-dependent cost with different value of $\alpha$}
	\label{fig:instance-dependent-cost-with-different-value-of-alpha}
\end{figure}

The instance-dependent cost is visualized for several values of $\alpha$ $\in$
[0.25, 4] in \ref{fig:instance-dependent-cost-with-different-value-of-alpha}. We note two properties of the instance-dependent cost.
(1) When an instance is misclassified and $p_t$ is small, the
prediction error term nears to 1, which contributes a large instance-dependent cost. As
$p_t$ nears to 1, the prediction error goes to 0 and the instance-dependent cost for well-classified instance is down-weighted to class-dependent cost. (2) The hypothesis parameter $\alpha$ smoothly adjusts the rate at which hard instances are focused on more. Hence, compare with the former definition of prediction error focusing the instances with $p_t$ nears to 0.5, the new defintion of prediction error focuses on the instances with $p_t$ nears to 0, which are considered as hard instances, and slightly adjusted by the hypothesis parameter $\alpha$.

The whole process with new structure is summarized in algorithm \ref{alg:Heuristics Instance-dependent Cost Online Classification}.

\begin{algorithm}
	\caption{Heuristics Instance-dependent Cost Online Classification 2}
	\KwIn{Input data, Input Labels}
	\KwOut{Prediction}
	\While{Have more samples}{
		datum, label <- next sample
		
		prediction label $\hat{y}$  <-  Predict by current classifier
		
		y <- the true label of this sample
		
		current ratio of Class y: $CRC_y$ <- $\frac{\# \ of\ current samples\ with\ label\ y}{\# \ of\ all\ current\ samples}$ 
		
		Class-dependent Cost: $CDC_y$ <- $\frac{1}{CRC_y}$
		
		
		\eIf{y is 1}{
			$p_t$  <- p
		}{
		   	$p_t$  <- 1 - p
		}
			prediction error <- $(1-p_t)^\alpha$ 
			
			Instance-dependent Cost: $IDC$ <- (1+prediction error ) $\times$ $CDC_y$ 
			
			
			train the classifier by fitting this instance with the heuristics instance-dependent cost
		
		
		
	}
\label{alg:Heuristics Instance-dependent Cost Online Classification}
\end{algorithm}



\section{Experiments}
\subsubsection{Experimental Setup}
In this subsection, we illustrate our experimental dataset setting, parameter setting, and comparison methods.

For comparison method, we design a naive version of our method, which does not further fine-tuning on the human designed cost matrix. The human designed matrix is totally determined by tracked imbalance ratio .

Following existing works of online class imbalance learning, we select Hoeffding Tree  as the base classifier and the number of base learners is set to $10$. We set the interval between each evolution $100$ data samples with a maximum generation of $5$ in each evolution process. The number of individuals are set to $20$ for all evolution processes. The metric we used for fitness evaluation is the difference between the maximum recall and the minimum recall.

One part of the datasets "contraceptive", "segment0", "yeast-0-2-5-6 vs 3-7-8-9" and "yeast1" come from Keel repository .
% Other real world datasets come from USP-DS repository \cite{souza2020challenges}. 
Other datasets comes from synthesizing. All the dataset information are summarized in Table 1. Here IR means the imbalance ratio (ratio between instance number of maximum class and instance number of minimum class).

The metric we used is online GMean following , which give instantaneous GMean for each time step. The overall GMean is calculated by simply averaging the GMean on every time steps.

\begin{table}[!ht] \tiny
	\centering
\begin{tabular}{|l||l|l|l||l||l|l|l||l||l|l|l|} 
	\hline
datasets    & IR    & \#ins. & \#fea. & datasets        & IR   & \#ins. & \#fea. & datasets      & IR   & \#ins. & \#fea. \\ \hline \hline
synthesize1 & 5.00  & 1200       & 10        & segment0        & 6.02 & 2308       & 19        & yeast1        & 2.46 & 1484       & 8         \\ \hline
synthesize2 & 5.00  & 1200       & 5         & segment0-5-1tra & 6.02 & 1846       & 19        & yeast1-5-1tra & 2.46 & 1187       & 8         \\ \hline
synthesize3 & 5.00  & 1200       & 10        & segment0-5-2tra & 6.02 & 1846       & 19        & yeast1-5-2tra & 2.46 & 1187       & 8         \\ \hline
synthesize4 & 10.00 & 1100       & 12        & segment0-5-3tra & 6.02 & 1846       & 19        & yeast1-5-3tra & 2.46 & 1187       & 8         \\ \hline
synthesize5 & 5.00  & 3000       & 25        & segment0-5-4tra & 6.00 & 1847       & 19        & yeast1-5-4tra & 2.46 & 1187       & 8         \\ \hline
synthesize6 & 5.00  & 3000       & 50        & segment0-5-5tra & 6.02 & 1847       & 19        & yeast1-5-5tra & 2.45 & 1188       & 8         \\ \hline
synthesize7 & 5.00  & 4800       & 50        & segment0-5-1tst & 6.00 & 462        & 19        & yeast1-5-1tst & 2.45 & 297        & 8         \\ \hline
synthesize8 & 5.00  & 4800       & 30        & segment0-5-2tst & 6.00 & 462        & 19        & yeast1-5-2tst & 2.45 & 297        & 8         \\ \hline
&       &            &           & segment0-5-3tst & 6.00 & 462        & 19        & yeast1-5-3tst & 2.45 & 297        & 8         \\ \hline
&       &            &           & segment0-5-4tst & 6.09 & 461        & 19        & yeast1-5-4tst & 2.45 & 297        & 8         \\ \hline
&       &            &           & segment0-5-5tst & 5.98 & 461        & 19        & yeast1-5-5tst & 2.48 & 296        & 8         \\ \hline
\end{tabular}
	\label{tab:overview}
	\caption{Overview of used datasets}
\end{table}



\subsubsection{Overall Performance Comparison}

\begin{table}[!ht]
	\centering
	\begin{tabular}{|c||c|ccccc|}
		\hline
		\multirow{2}{*}{datasets} & \multirow{2}{*}{class-dependent cost}  &  \multicolumn{5}{c|}{instance-dependent cost}                                                                                                                           \\ \cline{3-7} 
		&                                & \multicolumn{1}{c|}{$\alpha$=0.25}         & \multicolumn{1}{c|}{$\alpha$=0.5}          & \multicolumn{1}{c|}{$\alpha$=1}            & \multicolumn{1}{c|}{$\alpha$=2}            & $\alpha$=4            \\ \hline \hline
		synthesize1               & 0.897                                 & \multicolumn{1}{c|}{0.899}          & \multicolumn{1}{c|}{\textbf{0.901}} & \multicolumn{1}{c|}{0.899}          & \multicolumn{1}{c|}{0.899}          & 0.897          \\ \hline
		synthesize2               & 0.741                               & \multicolumn{1}{c|}{0.730}          & \multicolumn{1}{c|}{0.656}          & \multicolumn{1}{c|}{\textbf{0.751}} & \multicolumn{1}{c|}{\textbf{0.751}} & 0.745          \\ \hline
		synthesize3               & 0.831                                  & \multicolumn{1}{c|}{\textbf{0.841}} & \multicolumn{1}{c|}{0.818}          & \multicolumn{1}{c|}{0.809}          & \multicolumn{1}{c|}{0.827}          & 0.833          \\ \hline
		synthesize4               & 0.757                                & \multicolumn{1}{c|}{0.751}          & \multicolumn{1}{c|}{0.745}          & \multicolumn{1}{c|}{\textbf{0.771}} & \multicolumn{1}{c|}{0.713}          & 0.706          \\ \hline
		synthesize5               & 0.651                                 & \multicolumn{1}{c|}{0.629}          & \multicolumn{1}{c|}{0.683}          & \multicolumn{1}{c|}{0.571}          & \multicolumn{1}{c|}{0.684}          & \textbf{0.691} \\ \hline
		synthesize6               & 0.701                                 & \multicolumn{1}{c|}{0.715}          & \multicolumn{1}{c|}{0.659}          & \multicolumn{1}{c|}{\textbf{0.767}} & \multicolumn{1}{c|}{0.743}          & 0.754          \\ \hline
		synthesize7               & 0.730                                 & \multicolumn{1}{c|}{0.721}          & \multicolumn{1}{c|}{\textbf{0.774}} & \multicolumn{1}{c|}{0.751}          & \multicolumn{1}{c|}{0.768}          & 0.741          \\ \hline
		synthesize8               & 0.644                                 & \multicolumn{1}{c|}{0.686}          & \multicolumn{1}{c|}{0.704}          & \multicolumn{1}{c|}{0.699}          & \multicolumn{1}{c|}{\textbf{0.764}}          & 0.688          \\ \hline
	\end{tabular}
	\label{tab01:overall}
	\caption{Comparison with class-dependent cost in terms of overall GMean, the better algorithm is denoted as bold, synthesize benchmark}
\end{table}

\begin{table}[!ht]
	\centering
\begin{tabular}{|c||c|ccccc|}
	\hline
	\multirow{2}{*}{datasets} & \multirow{2}{*}{class-dependent cost} & \multicolumn{5}{c|}{instance-dependent cost}                                                                                                                           \\ \cline{3-7} 
	&                                       & \multicolumn{1}{c|}{$\alpha$=0.25}         & \multicolumn{1}{c|}{$\alpha$=0.5}          & \multicolumn{1}{c|}{$\alpha$=1}            & \multicolumn{1}{c|}{$\alpha$=2}            & $\alpha$=4            \\ \hline \hline
segment0                  & 0.957                                 & \multicolumn{1}{c|}{0.966}          & \multicolumn{1}{c|}{0.962}          & \multicolumn{1}{c|}{0.962}          & \multicolumn{1}{c|}{0.972}          & \textbf{0.973} \\ \hline
segment0-5-1tra           & 0.931                                 & \multicolumn{1}{c|}{0.948}          & \multicolumn{1}{c|}{\textbf{0.975}} & \multicolumn{1}{c|}{0.973}          & \multicolumn{1}{c|}{\textbf{0.975}} & \textbf{0.975} \\ \hline
segment0-5-2tra           & 0.931                                 & \multicolumn{1}{c|}{0.963}          & \multicolumn{1}{c|}{0.960}          & \multicolumn{1}{c|}{0.949}          & \multicolumn{1}{c|}{\textbf{0.968}} & 0.914          \\ \hline
segment0-5-3tra           & 0.956                                 & \multicolumn{1}{c|}{0.967}          & \multicolumn{1}{c|}{0.946}          & \multicolumn{1}{c|}{0.970}          & \multicolumn{1}{c|}{0.972}          & \textbf{0.973} \\ \hline
segment0-5-4tra           & 0.948                                 & \multicolumn{1}{c|}{0.929}          & \multicolumn{1}{c|}{0.946}          & \multicolumn{1}{c|}{0.946}          & \multicolumn{1}{c|}{\textbf{0.952}} & 0.950          \\ \hline
segment0-5-5tra           & 0.975                                 & \multicolumn{1}{c|}{\textbf{0.976}} & \multicolumn{1}{c|}{0.975}          & \multicolumn{1}{c|}{0.974}          & \multicolumn{1}{c|}{0.974}          & 0.974          \\ \hline
segment0-5-1tst           & 0.902                                 & \multicolumn{1}{c|}{0.870}          & \multicolumn{1}{c|}{0.887}          & \multicolumn{1}{c|}{\textbf{0.907}} & \multicolumn{1}{c|}{0.901}          & 0.892          \\ \hline
segment0-5-2tst           & 0.864                                 & \multicolumn{1}{c|}{\textbf{0.869}} & \multicolumn{1}{c|}{\textbf{0.869}} & \multicolumn{1}{c|}{\textbf{0.869}} & \multicolumn{1}{c|}{\textbf{0.869}} & \textbf{0.869} \\ \hline
segment0-5-3tst           & 0.821                                 & \multicolumn{1}{c|}{\textbf{0.886}} & \multicolumn{1}{c|}{0.883}          & \multicolumn{1}{c|}{0.848}          & \multicolumn{1}{c|}{0.848}          & 0.843          \\ \hline
segment0-5-4tst           & 0.874                                 & \multicolumn{1}{c|}{\textbf{0.922}} & \multicolumn{1}{c|}{0.908}          & \multicolumn{1}{c|}{0.913}          & \multicolumn{1}{c|}{0.903}          & 0.911          \\ \hline
segment0-5-5tst           & 0.861                                 & \multicolumn{1}{c|}{\textbf{0.866}} & \multicolumn{1}{c|}{\textbf{0.866}} & \multicolumn{1}{c|}{0.865}          & \multicolumn{1}{c|}{0.863}          & 0.863          \\ \hline
\end{tabular}
	\label{tab02:overall}
	\caption{Comparison with class-dependent cost in terms of overall GMean, the better algorithm is denoted as bold, segment benchmark}
\end{table}



\begin{table}[!ht]
	\centering
	\begin{tabular}{|c||c|ccccc|}
		\hline
		\multirow{2}{*}{datasets} & \multirow{2}{*}{class-dependent cost} & \multicolumn{5}{c|}{instance-dependent cost}                                                                                                                           \\ \cline{3-7} 
		&                                       & \multicolumn{1}{c|}{$\alpha$=0.25}         & \multicolumn{1}{c|}{$\alpha$=0.5}          & \multicolumn{1}{c|}{$\alpha$=1}            & \multicolumn{1}{c|}{$\alpha$=2}            & $\alpha$=4            \\ \hline \hline
	yeast1                    & 0.598                                 & \multicolumn{1}{c|}{0.661}          & \multicolumn{1}{c|}{0.655}          & \multicolumn{1}{c|}{0.636}      & \multicolumn{1}{c|}{0.643}      & \textbf{0.671} \\ \hline
	yeast1-5-1tra             & 0.553                                 & \multicolumn{1}{c|}{\textbf{0.570}} & \multicolumn{1}{c|}{0.537}          & \multicolumn{1}{c|}{0.537}      & \multicolumn{1}{c|}{0.529}      & 0.536          \\ \hline
	yeast1-5-2tra             & 0.617                                 & \multicolumn{1}{c|}{\textbf{0.623}} & \multicolumn{1}{c|}{0.601}          & \multicolumn{1}{c|}{0.610}      & \multicolumn{1}{c|}{0.619}      & 0.598          \\ \hline
	yeast1-5-3tra             & \textbf{0.623}                        & \multicolumn{1}{c|}{0.587}          & \multicolumn{1}{c|}{0.584}          & \multicolumn{1}{c|}{0.604}      & \multicolumn{1}{c|}{0.599}      & 0.585          \\ \hline
	yeast1-5-4tra             & 0.566                                 & \multicolumn{1}{c|}{0.570}          & \multicolumn{1}{c|}{\textbf{0.676}} & \multicolumn{1}{c|}{0.653}      & \multicolumn{1}{c|}{0.650}      & 0.651          \\ \hline
	yeast1-5-5tra             & \textbf{0.683}                        & \multicolumn{1}{c|}{0.647}          & \multicolumn{1}{c|}{0.598}          & \multicolumn{1}{c|}{0.620}      & \multicolumn{1}{c|}{0.572}      & 0.573          \\ \hline
	yeast1-5-1tst             & 0.602                                 & \multicolumn{1}{c|}{0.608}          & \multicolumn{1}{c|}{0.612}          & \multicolumn{1}{c|}{0.625}      & \multicolumn{1}{c|}{0.625}      & \textbf{0.637} \\ \hline
	yeast1-5-2tst             & \textbf{0.632}                        & \multicolumn{1}{c|}{0.619}          & \multicolumn{1}{c|}{0.591}          & \multicolumn{1}{c|}{0.589}      & \multicolumn{1}{c|}{0.629}      & 0.594          \\ \hline
	yeast1-5-3tst             & 0.455                                 & \multicolumn{1}{c|}{\textbf{0.471}} & \multicolumn{1}{c|}{0.433}          & \multicolumn{1}{c|}{0.447}      & \multicolumn{1}{c|}{0.449}      & 0.431          \\ \hline
	yeast1-5-4tst             & \textbf{0.554}                        & \multicolumn{1}{c|}{0.502}          & \multicolumn{1}{c|}{0.515}          & \multicolumn{1}{c|}{0.531}      & \multicolumn{1}{c|}{0.509}      & 0.480          \\ \hline
	yeast1-5-5tst             & 0.492                                 & \multicolumn{1}{c|}{\textbf{0.625}} & \multicolumn{1}{c|}{0.616}          & \multicolumn{1}{c|}{0.591}      & \multicolumn{1}{c|}{0.581}      & 0.578          \\ \hline
\end{tabular}
	\label{tab03:overall}
	\caption{Comparison with class-dependent cost in terms of overall GMean, the better algorithm is denoted as bold, yeast benchmark}
\end{table}

We conduct the experiment on aforementioned datasets, the performances of our method and comparison method are summarized in Table 2.


For the second aspect, the prediction error is redefined as: 

\begin{equation}
	\text{prediction error} = (1 - p_t)^\alpha,
\end{equation}
where $p_t$ is the classification probability as the true label, defined as:

\begin{equation}
	p_t = 
	\begin{cases}
		p, \ \text{if y=1}\\
		1-p, \ \text{if y=0}\\
	\end{cases}
\end{equation}
In the above y $\in$ $\{ 0, 1\}$ specifies the ground-truth class and p $\in$ [0, 1] is the model's estimated probability for the class with label y = 1.

Then the instance-dependent cost is redefined as 
\begin{equation}
	\text{instance-dependent cost}= (\text{threshold}+ \text{prediction error})\times \text{class-dependent cost}
\end{equation}

$$\hat{y}_t =arg\ \text{max} \Theta_{t-1}(X_t)=arg\ \text{max} \sum_{i=1}^{42} \text{weight}_{i,t} \times \theta_{i,t-1}(X_t)$$
where $\text{weight}_{i,t}=\frac{\text{gmean}_{i,t-1}}{\sum_{j=1}^{42}\text{gmean}_{j,t-1}}$ 

\begin{table}[!ht]
	\centering
	\begin{tabular}{|c||c|c|ccccc|}
		\hline
		\multirow{2}{*}{datasets} & \multirow{2}{*}{class-dependent cost}  & \multirow{2}{*}{ensemble} & \multicolumn{5}{c|}{instance-dependent cost}                                                                                                                           \\ \cline{4-8} 
		&          &                             & \multicolumn{1}{c|}{$\alpha$=0.25}         & \multicolumn{1}{c|}{$\alpha$=0.5}          & \multicolumn{1}{c|}{$\alpha$=1}            & \multicolumn{1}{c|}{$\alpha$=2}            & $\alpha$=4            \\ \hline \hline
		synthesize1               & 0.897     & \textbf{0.902}                             & \multicolumn{1}{c|}{0.899}          & \multicolumn{1}{c|}{0.901} & \multicolumn{1}{c|}{0.899}          & \multicolumn{1}{c|}{0.899}          & 0.897          \\ \hline
		synthesize2               & 0.741     & 0.744                             & \multicolumn{1}{c|}{0.730}          & \multicolumn{1}{c|}{0.656}          & \multicolumn{1}{c|}{\textbf{0.751}} & \multicolumn{1}{c|}{\textbf{0.751}} & 0.745          \\ \hline
		synthesize3               & 0.831      & \textbf{0.855}                              & \multicolumn{1}{c|}{0.841} & \multicolumn{1}{c|}{0.818}          & \multicolumn{1}{c|}{0.809}          & \multicolumn{1}{c|}{0.827}          & 0.833          \\ \hline
		synthesize4               & 0.757         & \textbf{0.791}                          & \multicolumn{1}{c|}{0.751}          & \multicolumn{1}{c|}{0.745}          & \multicolumn{1}{c|}{0.771} & \multicolumn{1}{c|}{0.713}          & 0.706          \\ \hline
		synthesize5               & 0.651         & \textbf{0.712}                           & \multicolumn{1}{c|}{0.629}          & \multicolumn{1}{c|}{0.683}          & \multicolumn{1}{c|}{0.571}          & \multicolumn{1}{c|}{0.684}          & 0.691 \\ \hline
		synthesize6               & 0.701         & \textbf{0.771}                           & \multicolumn{1}{c|}{0.715}          & \multicolumn{1}{c|}{0.659}          & \multicolumn{1}{c|}{0.767} & \multicolumn{1}{c|}{0.743}          & 0.754          \\ \hline
		synthesize7               & 0.730          & \textbf{0.806}                         & \multicolumn{1}{c|}{0.721}          & \multicolumn{1}{c|}{0.774} & \multicolumn{1}{c|}{0.751}          & \multicolumn{1}{c|}{0.768}          & 0.741          \\ \hline
		synthesize8               & 0.644            & \textbf{0.769}                        & \multicolumn{1}{c|}{0.686}          & \multicolumn{1}{c|}{0.704}          & \multicolumn{1}{c|}{0.699}          & \multicolumn{1}{c|}{0.764}          & 0.688          \\ \hline
	\end{tabular}
	\label{tab04:overall}
	\caption{Comparison with class-dependent cost in terms of overall GMean, the better algorithm is denoted as bold, synthesize benchmark}
\end{table}
\section{Conclusion}
Then the instance-dependent cost is redefined as 
\begin{equation}
	\text{instance-dependent cost}= (\text{threshold}+ \text{prediction error})\times \text{class-dependent cost}
\end{equation}

$$\hat{y}_t =\mathrm{arg\ }  \underset{Label}{\mathrm{max}} \mathbf{\Theta}_{t-1}(\mathbf{X}_t)=\mathrm{arg\ }  \underset{Label}{\mathrm{max}}  \sum_{i=1}^{42} w_{i,t} \mathbf{\Theta}_{i,t-1}(\mathbf{X}_t)$$
where $w_{i,t}=\frac{\text{gmean}_{i,t-1}}{\sum_{j=1}^{42}\text{gmean}_{j,t-1}}$, $\mathbf{\Theta}_{i,t-1}$ is the $i^{th}$ base classifier obtained at timestep $t-1$, $\mathbf{\Theta}_{i,t-1}(\mathbf{X}_t)$ outputs a vector containing the probability distribution of each predicted label.


\begin{algorithm}
	\caption{Mass and Elite Strategy Based Instance-dependent Cost Online Classification(MAESIC)}
	\KwIn{Input data, Input Labels}
	\KwOut{Prediction}
	Initialization: $\gamma\leftarrow \mathrm{decay\ rate}$, $N \leftarrow \mathrm{num\ of \ baseclassifier}$
	%, $\mathrm{threshold} \leftarrow \{0.0, 0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8\}$ $\alpha %\leftarrow \{ 0.125, 0.25, 0.5, 1, 2, 4, 8 \}$
	
	\While{Have more samples}{
		$t \leftarrow \mathrm{timestep}$  
		
		datum, label $\leftarrow$ next sample
		
		$\rho$ ← a random number $\in$ [0, 1]
		
		%\eIf{$\rho <e^{-\frac{\varDelta f\left( X \right)}{T_n}} $ }{
		\eIf{$\rho <e^{-\gamma t}$ }{
		 	$\mathbb{S} \leftarrow \{1,2,...,N\}$    //mass strategy
		}
	        {
	        	$\mathbb{S} \leftarrow \mathrm{arg\ } \underset{\text{base indices}, j=1,2,..N }{\max_{\substack{i_1, i_2, \ldots, i_K \\ }}} \text{gmean}_{j,t-1}$        	
	        	//elite strategy
	    }
        $w_{i,t}\leftarrow \frac{\text{gmean}_{i,t-1}}{\underset{j \in \mathbb{S}}{\sum}\text{gmean}_{j,t-1}}, i \in \mathbb{S}$ 
        
        $\hat{y}_t \leftarrow \mathrm{arg\ }  \underset{Label}{\mathrm{max}} \mathbf{\Theta}_{t-1}(\mathbf{X}_t)=\mathrm{arg\ }  \underset{Label}{\mathrm{max}}  \underset{i \in \mathbb{S}}{\sum} w_{i,t} \mathbf{\Theta}_{i,t-1}(\mathbf{X}_t)$
        
        
		
		$y_t \leftarrow$ the true label of this sample
		
		 
		 
		 
		update current ratio of class $y_t$: $R_{y_t} \leftarrow \frac{\text{\# of current samples with label}\ y_t}{\text{\# of all current samples}}$ 
		
		class-dependent cost  $\leftarrow \frac{1}{R_{y_t}}$
		
		$p_{t,i}\leftarrow [\mathbf{\Theta}_{i,t-1}(\mathbf{X}_t)]_{y_t},i \in \{1,2,..,N\}$ 
		
		$\text{prediction error}_i \leftarrow (1-p_{t,i})^{\alpha_i},i \in \{1,2,..,N\}$
		
		$\text{instance-dependent cost}_i\leftarrow(\text{threshold}_i+\text{prediction error}_i)\text{class-dependent cost},i \in \{1,2,..,N\}$ 
		
		$\mathbf{\Theta}_{i,t} \leftarrow \text{train}(\mathbf{\Theta}_{i,t-1},\text{instance-dependent cost}_i),i \in \{1,2,..,N\}$
	
		
		
	}
	\label{alg:Mass and Elite Strategy Based Instance-dependent Cost Online Classification}
	Note that $\mathbf{\Theta}_{i,t-1}$ is the $i^{th}$ base classifier obtained at timestep $t-1$, $\mathbf{\Theta}_{i,t-1}(\mathbf{X}_t)$ outputs a vector containing the probability distribution of each predicted label
\end{algorithm}
\begin{table}[!ht]
	\centering
	\begin{tabular}{|c||c|cc|}
		\hline
		\multirow{2}{*}{datasets} & \multirow{2}{*}{class-dependent cost}  &\multicolumn{2}{c|}{instance-dependent cost}                                                                                                                           \\ \cline{3-4} 
		&                                      & \multicolumn{1}{c|}{Mass Strategy}                     & Mass and Elite Strategy            \\ \hline \hline
		synthesize1               & 0.897                            & \multicolumn{1}{c|}{\textbf{0.907}}          & \textbf{0.907}          \\ \hline
		synthesize2               & 0.741                        & \multicolumn{1}{c|}{0.799}    & \textbf{0.820}         \\ \hline
		synthesize3               & 0.831                         & \multicolumn{1}{c|}{\textbf{0.851}}   & 0.848          \\ \hline
		synthesize4               & 0.757                        & \multicolumn{1}{c|}{\textbf{0.773}}           & 0.758          \\ \hline
		synthesize5               & 0.651                      & \multicolumn{1}{c|}{0.734}          & \textbf{0.744} \\ \hline
		synthesize6               & 0.701                          & \multicolumn{1}{c|}{\textbf{0.793}}            & 0.780          \\ \hline
		synthesize7               & 0.730                   & \multicolumn{1}{c|}{\textbf{0.825}}               & 0.820          \\ \hline
		synthesize8               & 0.644             & \multicolumn{1}{c|}{0.778}            & \textbf{0.796}          \\ \hline
	\end{tabular}
	\label{tab5:overall}
	\caption{Comparison with class-dependent cost in terms of overall GMean, the better algorithm is denoted as bold, synthesize benchmark}
\end{table}
\end{document}
